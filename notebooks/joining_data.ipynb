{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the data from Part 1 with the data from Part 2 to create a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# File paths for your CSV files\n",
    "csv_files = [\n",
    "    'D:/Documents/GitHub/Statistical-Modelling-Project/notebooks/foursquare_data.csv',\n",
    "    'D:/Documents/GitHub/Statistical-Modelling-Project/notebooks/yelp_data.csv',\n",
    "    'D:/Documents/GitHub/Statistical-Modelling-Project/notebooks/montreal_df.csv'\n",
    "]\n",
    "\n",
    "# Function to create SQLite database and tables\n",
    "def create_db_and_tables():\n",
    "    conn = sqlite3.connect('Montreal_POI_BIKE.db')  # Connecting to SQLite database\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create tables for each CSV file\n",
    "    for csv_file in csv_files:\n",
    "        table_name = csv_file.split('/')[-1].split('.')[0]  # Extract table name from file path\n",
    "        df = pd.read_csv(csv_file)  # Reading CSV into DataFrame\n",
    "\n",
    "        # Convert DataFrame to SQL table\n",
    "        df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "\n",
    "#Confirm that the Table was created \n",
    "        print(f\"Table '{table_name}' created successfully.\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "# in researching for method to combine these dataframes found this would limit the execution of the code to prevent it from \n",
    "#being run automatically. This may cause data to be overwritten which could erase steps to clean data. \n",
    "if __name__ == \"__main__\":\n",
    "    create_db_and_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a visualization that you used as part of your EDA process. Explain the initial pattern or relationship you discoved through this visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create a heatmap\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m      3\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(data\u001b[38;5;241m.\u001b[39mcorr(), annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoolwarm\u001b[39m\u001b[38;5;124m'\u001b[39m, center\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrelation Heatmap\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a heatmap\n",
    "#This code was run in the EDA notebook\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(data.corr(), annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put all your results in an SQLite3 database (remember, SQLite stores its databases as files in your local machine - make sure to create your database in your project's data/ directory!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above was used to create the database using a function. Stored CSV files into Montreal_POI_BIKE.db as tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the data before and after the join to validate your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of checking data before joining and altering\n",
    "\n",
    "conn = sqlite3.connect('D:/Documents/GitHub/Statistical-Modelling-Project/notebooks/Montreal_POI_BIKE.db')\n",
    "\n",
    "#Reading tables into pandas DataFrames\n",
    "montreal_data = pd.read_sql_query(\"SELECT * FROM montreal_df;\", conn)\n",
    "foursquare_data = pd.read_sql_query(\"SELECT * FROM foursquare_data;\", conn)\n",
    "yelp_data = pd.read_sql_query(\"SELECT * FROM yelp_data;\", conn)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checked how each column fit in table. \n",
    "#then changed names to better match each table\n",
    "montreal_data.head()\n",
    "foursquare_data.head()\n",
    "yelp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#change colum names to fit the other tables\n",
    "montreal_data = montreal_data.rename(columns={\n",
    "    'name': 'Bike Station Name',\n",
    "    'latitude': 'Latitude',\n",
    "    'longitude': 'Longitude',\n",
    "    'free_bikes': 'Free_Bikes',\n",
    "    'empty_slots': 'Empty_Slots'\n",
    "    })\n",
    "\n",
    "foursquare_data.rename(columns={\n",
    "    'Name': 'Venue Name'\n",
    "      })\n",
    "yelp_data.rename(columns={\n",
    "    'Name': 'Venue Name'\n",
    "      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge montreal_data with foursquare_data\n",
    "merged_df = pd.merge(montreal_data, foursquare_data, on=['Latitude', 'Longitude'], how='left')\n",
    "\n",
    "# Merge montreal_data with yelp_data\n",
    "merged_df = pd.merge(merged_df, yelp_data, on=['Latitude', 'Longitude'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with no values (all null)\n",
    "columns_to_drop = ['Name_x', 'Address_x', 'Categories', 'Rating_x', 'TotalRatings']\n",
    "merged_df = merged_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Display the remaining columns\n",
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'Name_y', 'Rating_y', 'Address_y' are not null\n",
    "merged_df = merged_df[merged_df['Name_y'].notnull() & \n",
    "                      merged_df['Rating_y'].notnull() & \n",
    "                      merged_df['Address_y'].notnull()]\n",
    "\n",
    "# Display the remaining rows\n",
    "print(merged_df.isna().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
